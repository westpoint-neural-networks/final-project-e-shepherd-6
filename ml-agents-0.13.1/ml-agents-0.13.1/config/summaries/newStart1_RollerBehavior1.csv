Steps,Policy/Entropy,Policy/Learning Rate,Policy/Extrinsic Value Estimate,Policy/Extrinsic Reward,Environment/Cumulative Reward,Environment/Episode Length,Losses/Value Loss,Losses/Policy Loss
1000,1.421688,0.00029998503,-0.016366784,-0.03580476537445065,-0.0358047653809308,278.9375,0.01017451,0.25436392
2000,1.424846,0.00029995505,0.023566535,-0.15684563600841692,-0.1568456385309272,350.70588235294116,0.004316734,0.25251696
3000,1.4177421,0.00029992504,-0.01935578,-0.34839268538224344,-0.34839268476646207,354.47058823529414,0.0030583348,0.2519822
4000,1.4033343,0.000299895,-0.03475602,-0.17665318096987903,-0.17665318126137208,372.375,0.005962902,0.25738952
5000,1.405144,0.000299865,-0.03839555,-0.2869041762314737,-0.28690418195674283,463.25,0.0004490668,0.2538781
6000,1.4058698,0.00029983502,-0.029797448,-0.2284115695466216,-0.22841157188276368,478.6923076923077,0.000898034,0.2483696
7000,1.4049623,0.00029980502,-0.027887134,-0.15637322275766305,-0.15637322312172078,457.35714285714283,0.00017791262,0.25655296
8000,1.4031308,0.00029977504,-0.032416537,-0.14638462082411235,-0.14638462473983685,442.0,7.091424e-05,0.24744034
